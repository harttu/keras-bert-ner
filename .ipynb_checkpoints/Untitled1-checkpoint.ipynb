{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "from tensorflow import keras\n",
    "\n",
    "import numpy as np\n",
    "import conlleval\n",
    "\n",
    "from common import encode, label_encode, write_result\n",
    "from common import load_pretrained, viterbi_probabilities\n",
    "from common import create_ner_model, create_optimizer, argument_parser\n",
    "from common import read_conll, process_sentences, get_labels\n",
    "from common import save_ner_model, save_viterbi_probabilities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Heavier part, loading the pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "argv = ['/users/htoivone/keras-bert-ner/scripts/../ner_v2.py', \n",
    "        '--vocab_file', '/scratch/project_2001426/models/biobert_v1.1_pubmed_std_naming/vocab.txt', \n",
    "        '--bert_config_file', '/scratch/project_2001426/models/biobert_v1.1_pubmed_std_naming/bert_config.json', \n",
    "        '--init_checkpoint', '/scratch/project_2001426/models/biobert_v1.1_pubmed_std_naming/bert_model.ckpt', \n",
    "        '--learning_rate', '5e-5', \n",
    "        '--num_train_epochs', '3', \n",
    "        '--max_seq_length', '256', \n",
    "        '--batch_size', '4', \n",
    "        '--train_data', '/users/htoivone/links/august/data/chemdner-smaller/conll/train.tsv', \n",
    "        '--test_data', '/users/htoivone/links/august/data/chemdner-smaller/conll/test.tsv', \n",
    "        '--ner_model_dir', '/users/htoivone/keras-bert-ner/scripts/../ner-models/testi_1']\n",
    "\n",
    "\n",
    "argparser = argument_parser()\n",
    "args = argparser.parse_args(argv[1:])\n",
    "seq_len = args.max_seq_length    # abbreviation\n",
    "pretrained_model, tokenizer = load_pretrained(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_words, train_tags = read_conll(args.train_data)\n",
    "test_words, test_tags = read_conll(args.test_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2972,)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(train_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Chronic', 'administration', 'of', 'haloperidol', 'increased', 'Dpp6', 'expression', 'in', 'mouse', 'brains', '.']\n",
      "['DPP6', 'is', 'an', 'auxiliary', 'subunit', 'of', 'Kv4', 'and', 'regulates', 'the', 'properties', 'of', 'Kv4', ',', 'which', 'regulates', 'the', 'activity', 'of', 'dopaminergic', 'neurons', '.']\n",
      "['The', 'findings', 'of', 'this', 'study', 'indicate', 'that', 'an', 'altered', 'response', 'of', 'Kv4', '/', 'DPP6', 'to', 'long', '-', 'term', 'neuroleptic', 'administration', 'is', 'involved', 'in', 'neuroleptic', '-', 'induced', 'TD', '.']\n",
      "['Nanosilver', 'effects', 'on', 'growth', 'parameters', 'in', 'experimental', 'aflatoxicosis', 'in', 'broiler', 'chickens', '.']\n"
     ]
    }
   ],
   "source": [
    "for tw in train_words[6:10]:\n",
    "    print(tw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['O', 'O', 'O', 'B-Chemical', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n"
     ]
    }
   ],
   "source": [
    "for tt in train_tags[6:8]:\n",
    "    print(tt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = process_sentences(train_words, train_tags, tokenizer, seq_len)\n",
    "test_data = process_sentences(test_words, test_tags, tokenizer, seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Chronic',\n",
       " 'administration',\n",
       " 'of',\n",
       " 'haloperidol',\n",
       " 'increased',\n",
       " 'Dpp6',\n",
       " 'expression',\n",
       " 'in',\n",
       " 'mouse',\n",
       " 'brains',\n",
       " '.']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0][6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ch\t\t\t\tO\n",
      "##ronic\t\t\t\tO\n",
      "administration\t\t\t\tO\n",
      "of\t\t\t\tO\n",
      "ha\t\t\t\tB-Chemical\n",
      "##lop\t\t\t\tI-Chemical\n",
      "##eri\t\t\t\tI-Chemical\n",
      "##do\t\t\t\tI-Chemical\n",
      "##l\t\t\t\tI-Chemical\n",
      "increased\t\t\t\tO\n",
      "D\t\t\t\tO\n",
      "##pp\t\t\t\tO\n",
      "##6\t\t\t\tO\n",
      "expression\t\t\t\tO\n",
      "in\t\t\t\tO\n",
      "mouse\t\t\t\tO\n",
      "brains\t\t\t\tO\n",
      ".\t\t\t\tO\n"
     ]
    }
   ],
   "source": [
    "for td,tt in zip(train_data[1][6],train_data[2][6]):\n",
    "    print(td+\"\\t\\t\\t\\t\"+tt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[3][6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "246"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data[4][6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ch', '##ronic', 'administration', 'of', 'ha', '##lop', '##eri', '##do', '##l', 'increased', 'D', '##pp', '##6', 'expression', 'in', 'mouse', 'brains', '.', '[SEP]', 'D', '##PP', '##6', 'is', 'an', 'auxiliary', 'subunit', 'of', 'K', '##v', '##4', 'and', 'regulate', '##s', 'the', 'properties', 'of', 'K', '##v', '##4', ',', 'which', 'regulate', '##s', 'the', 'activity', 'of', 'do', '##pa', '##mine', '##rg', '##ic', 'neurons', '.', '[SEP]', 'The', 'findings', 'of', 'this', 'study', 'indicate', 'that', 'an', 'altered', 'response', 'of', 'K', '##v', '##4', '/', 'D', '##PP', '##6', 'to', 'long', '-', 'term', 'ne', '##uro', '##le', '##ptic', 'administration', 'is', 'involved', 'in', 'ne', '##uro', '##le', '##ptic', '-', 'induced', 'TD', '.', '[SEP]', 'Nan', '##os', '##il', '##ver', 'effects', 'on', 'growth', 'parameters', 'in', 'experimental', 'a', '##f', '##lat', '##ox', '##ico', '##sis', 'in', 'br', '##oil', '##er', 'chickens', '.', '[SEP]', 'A', '##f', '##lat', '##ox', '##ico', '##sis', 'is', 'a', 'cause', 'of', 'economic', 'losses', 'in', 'br', '##oil', '##er', 'production', '.', '[SEP]', 'In', 'this', 'study', ',', 'the', 'effect', 'of', 'one', 'commercial', 'na', '##no', '##com', '##po', '##und', ',', 'Nan', '##oc', '##id', '(', 'Nan', '##o', 'Na', '##s', '##b', 'Pa', '##rs', 'Co', '.', ',', 'Iran', ')', 'was', 'evaluated', 'in', 'reduction', 'of', 'a', '##f', '##lat', '##ox', '##in', 'effects', 'on', 'the', 'growth', 'and', 'performance', 'in', '##dices', 'in', 'br', '##oil', '##er', 'chickens', 'suffering', 'from', 'experimental', 'a', '##f', '##lat', '##ox', '##ico', '##sis', '.', '[SEP]', 'For', 'this', ',', 'a', 'total', 'of', '300', 'one', '-', 'day', '-', 'old', 'br', '##oil', '##er', 'chick', '##s', '(', 'Ross', 'strain', ')', 'were', 'randomly', 'divided', 'into', '4', 'groups', 'with', '3', 'replica', '##tes', 'of', '15', 'chick', '##s', 'in', 'each', 'separated', 'pen', 'during', 'the', '28', '-', 'day', 'experiment', '.']\n",
      "['O', 'O', 'O', 'O', 'B-Chemical', 'I-Chemical', 'I-Chemical', 'I-Chemical', 'I-Chemical', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', '[SEP]', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', '[SEP]', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', '[SEP]', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', '[SEP]', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', '[SEP]', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Chemical', 'I-Chemical', 'I-Chemical', 'I-Chemical', 'I-Chemical', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', '[SEP]', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n"
     ]
    }
   ],
   "source": [
    "for td in train_data[4:7]:\n",
    "    print(td[6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['O', 'I-Chemical', 'B-Chemical', '[SEP]', '[PAD]']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_list = get_labels(train_data.labels)\n",
    "label_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'O': 0, 'I-Chemical': 1, 'B-Chemical': 2, '[SEP]': 3, '[PAD]': 4}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag_map = { l: i for i, l in enumerate(label_list) }\n",
    "tag_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'O', 1: 'I-Chemical', 2: 'B-Chemical', 3: '[SEP]', 4: '[PAD]'}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inv_tag_map = { v: k for k, v in tag_map.items() }\n",
    "inv_tag_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_prob, trans_prob = viterbi_probabilities(train_data.labels, tag_map)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.94448183, 0.        , 0.05551817, 0.        , 0.        ])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "init_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.97498724, 0.        , 0.02501276, 0.        , 0.        ],\n",
       "       [0.23771207, 0.76044595, 0.00184198, 0.        , 0.        ],\n",
       "       [0.09586535, 0.90413465, 0.        , 0.        , 0.        ],\n",
       "       [0.2       , 0.2       , 0.2       , 0.2       , 0.2       ],\n",
       "       [0.2       , 0.2       , 0.2       , 0.2       , 0.2       ]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trans_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = encode(train_data.combined_tokens, tokenizer, seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 2972, 256)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(train_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(256,)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(train_x[0][6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "256"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_x[0][6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  101, 20394, 26003,  3469,  1104,  5871, 13200,  9866,  2572,\n",
       "        1233,  2569,   141,  8661,  1545,  2838,  1107, 10322, 16570,\n",
       "         119,   102,   141, 20923,  1545,  1110,  1126, 13817, 27555,\n",
       "        1104,   148,  1964,  1527,  1105, 16146,  1116,  1103,  4625,\n",
       "        1104,   148,  1964,  1527,   117,  1134, 16146,  1116,  1103,\n",
       "        3246,  1104,  1202,  4163,  9685, 10805,  1596, 16993,   119,\n",
       "         102,  1109,  9505,  1104,  1142,  2025,  5057,  1115,  1126,\n",
       "        8599,  2593,  1104,   148,  1964,  1527,   120,   141, 20923,\n",
       "        1545,  1106,  1263,   118,  1858, 24928, 11955,  1513,  8956,\n",
       "        3469,  1110,  2017,  1107, 24928, 11955,  1513,  8956,   118,\n",
       "       10645, 15439,   119,   102, 20689,  2155,  2723,  4121,  3154,\n",
       "        1113,  3213, 11934,  1107,  6700,   170,  2087, 16236, 10649,\n",
       "       10658,  4863,  1107,  9304, 20708,  1200, 26199,   119,   102,\n",
       "         138,  2087, 16236, 10649, 10658,  4863,  1110,   170,  2612,\n",
       "        1104,  2670,  6053,  1107,  9304, 20708,  1200,  1707,   119,\n",
       "         102,  1130,  1142,  2025,   117,  1103,  2629,  1104,  1141,\n",
       "        2595,  9468,  2728,  8178,  5674,  6775,   117, 20689, 13335,\n",
       "        2386,   113, 20689,  1186, 11896,  1116,  1830, 19585,  1733,\n",
       "        3291,   119,   117,  3398,   114,  1108, 17428,  1107,  7234,\n",
       "        1104,   170,  2087, 16236, 10649,  1394,  3154,  1113,  1103,\n",
       "        3213,  1105,  2099,  1107, 28092,  1107,  9304, 20708,  1200,\n",
       "       26199,  5601,  1121,  6700,   170,  2087, 16236, 10649, 10658,\n",
       "        4863,   119,   102,  1370,  1142,   117,   170,  1703,  1104,\n",
       "        3127,  1141,   118,  1285,   118,  1385,  9304, 20708,  1200,\n",
       "       22282,  1116,   113,  5104, 10512,   114,  1127, 19729,  3233,\n",
       "        1154,   125,  2114,  1114,   124, 16498,  3052,  1104,  1405,\n",
       "       22282,  1116,  1107,  1296,  4757,  8228,  1219,  1103,  1743,\n",
       "         118,  1285,  7886,   119,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x[0][6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x = encode(test_data.combined_tokens, tokenizer, seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y, train_weights = label_encode(\n",
    "    train_data.combined_labels, tag_map, seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2972, 256, 1)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "test_y, test_weights = label_encode(\n",
    "    test_data.combined_labels, tag_map, seq_len)\n",
    "\n",
    "ner_model = create_ner_model(pretrained_model, len(tag_map))\n",
    "optimizer = create_optimizer(len(train_x[0]), args)\n",
    "\n",
    "ner_model.compile(\n",
    "    optimizer,\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    sample_weight_mode='temporal',\n",
    "    metrics=['sparse_categorical_accuracy']\n",
    ")\n",
    "\n",
    "callbacks_list = [\n",
    "keras.callbacks.EarlyStopping(monitor='val_loss',patience=1,),\n",
    "keras.callbacks.ModelCheckpoint(filepath=args.ner_model_dir+'/model.hdf5',\n",
    "monitor='val_loss',save_best_only=True,)]\n",
    "\n",
    "if args.early_stopping:\n",
    "    print(\"NER:Using early stopping.\")\n",
    "    ner_model.fit(\n",
    "    train_x,train_y,sample_weight=train_weights,epochs=args.num_train_epochs,\n",
    "    batch_size=args.batch_size,callbacks=callbacks_list,validation_data=(test_x,test_y))\n",
    "else:\n",
    "    ner_model.fit(train_x,train_y,sample_weight=train_weights,\n",
    "    epochs=args.num_train_epochs,batch_size=args.batch_size,)\n",
    "\n",
    "if args.ner_model_dir is not None:\n",
    "    label_list = [v for k, v in sorted(list(inv_tag_map.items()))]\n",
    "    save_ner_model(ner_model, tokenizer, label_list, args)\n",
    "    save_viterbi_probabilities(init_prob, trans_prob, inv_tag_map, args)\n",
    "\n",
    "probs = ner_model.predict(test_x, batch_size=args.batch_size)\n",
    "preds = np.argmax(probs, axis=-1)\n",
    "\n",
    "pred_tags = []\n",
    "for i, pred in enumerate(preds):\n",
    "    pred_tags.append([inv_tag_map[t]\n",
    "                      for t in pred[1:len(test_data.tokens[i])+1]])\n",
    "\n",
    "lines = write_result(\n",
    "    args.output_file, test_data.words, test_data.lengths,\n",
    "    test_data.tokens, test_data.labels, pred_tags\n",
    ")\n",
    "\n",
    "c = conlleval.evaluate(lines)\n",
    "conlleval.report(c)\n",
    "#return 0\n",
    "\n",
    "\n",
    "#if __name__ == '__main__':\n",
    "#    sys.exit(main(sys.argv))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    'batch_size':4, \n",
    "    'bert_config_file':'/users/htoivone/links/august/scripts/../models/bert-base-finnish-cased-v1/bert_config.json',\n",
    "    'dev_data':None, \n",
    "    'do_lower_case':False, \n",
    "    'early_stopping':False, \n",
    "    'init_checkpoint':'/users/htoivone/links/august/scripts/../models/bert-base-finnish-cased-v1/bert_model.ckpt', \n",
    "    'learning_rate':5e-05, \n",
    "    'max_seq_length':128, \n",
    "    'ner_model_dir':'/users/htoivone/links/august/scripts/../ner-models/turku-ner2-model', \n",
    "    'num_train_epochs':4, \n",
    "    'output_file':'output.tsv', \n",
    "    'test_data':'/users/htoivone/links/august/scripts/../data/AnatEM-1.0.2/conll_single_class//test.tsv', \n",
    "    'train_data':'/users/htoivone/links/august/scripts/../data/AnatEM-1.0.2/conll_single_class//train.tsv', \n",
    "    'viterbi':False, \n",
    "    'vocab_file':'/users/htoivone/links/august/scripts/../models/bert-base-finnish-cased-v1/vocab.txt', \n",
    "    'warmup_proportion':0.1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import Namespace\n",
    "\n",
    "args = Namespace(\n",
    "    batch_size=4, \n",
    "    bert_config_file='/users/htoivone/links/august/scripts/../models/bert-base-finnish-cased-v1/bert_config.json',\n",
    "    dev_data=None, \n",
    "    do_lower_case=False, \n",
    "    early_stopping=False, \n",
    "    init_checkpoint='/users/htoivone/links/august/scripts/../models/bert-base-finnish-cased-v1/bert_model.ckpt', \n",
    "    learning_rate=5e-05, \n",
    "    max_seq_length=128, \n",
    "    ner_model_dir='/users/htoivone/links/august/scripts/../ner-models/turku-ner2-model', \n",
    "    num_train_epochs=4, \n",
    "    output_file='output.tsv', \n",
    "    test_data='/users/htoivone/links/august/scripts/../data/AnatEM-1.0.2/conll_single_class//test.tsv', \n",
    "    train_data='/users/htoivone/links/august/scripts/../data/AnatEM-1.0.2/conll_single_class//train.tsv', \n",
    "    viterbi=False, \n",
    "    vocab_file='/users/htoivone/links/august/scripts/../models/bert-base-finnish-cased-v1/vocab.txt', \n",
    "    warmup_proportion=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.viterbi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-3e629c295f3b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mseq_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_seq_length\u001b[0m    \u001b[0;31m# abbreviation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mpretrained_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/keras-bert-ner/common.py\u001b[0m in \u001b[0;36mload_pretrained\u001b[0;34m(options)\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m         \u001b[0mseq_len\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_seq_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m     )\n\u001b[1;32m    103\u001b[0m     tokenizer = tokenization.FullTokenizer(\n",
      "\u001b[0;32m~/keras-bert-ner/venv/lib/python3.7/site-packages/keras_bert/loader.py\u001b[0m in \u001b[0;36mload_trained_model_from_checkpoint\u001b[0;34m(config_file, checkpoint_file, training, trainable, output_layer_num, seq_len, **kwargs)\u001b[0m\n\u001b[1;32m    167\u001b[0m         \u001b[0moutput_layer_num\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_layer_num\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0mseq_len\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseq_len\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m         **kwargs)\n\u001b[0m\u001b[1;32m    170\u001b[0m     \u001b[0mload_model_weights_from_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheckpoint_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/keras-bert-ner/venv/lib/python3.7/site-packages/keras_bert/loader.py\u001b[0m in \u001b[0;36mbuild_model_from_config\u001b[0;34m(config_file, training, trainable, output_layer_num, seq_len, **kwargs)\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0moutput_layer_num\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_layer_num\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m         **kwargs)\n\u001b[0m\u001b[1;32m     59\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/keras-bert-ner/venv/lib/python3.7/site-packages/keras_bert/bert.py\u001b[0m in \u001b[0;36mget_model\u001b[0;34m(token_num, pos_num, seq_len, embed_dim, transformer_num, head_num, feed_forward_dim, dropout_rate, attention_activation, feed_forward_activation, training, trainable, output_layer_num, use_task_embed, task_num)\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0membed_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0membed_dim\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0mpos_num\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpos_num\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m         \u001b[0mdropout_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdropout_rate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m     )\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0muse_task_embed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/keras-bert-ner/venv/lib/python3.7/site-packages/keras_bert/layers/embedding.py\u001b[0m in \u001b[0;36mget_embedding\u001b[0;34m(inputs, token_num, pos_num, embed_dim, dropout_rate, trainable)\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Embedding-Token'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         )(inputs[0]),\n\u001b[0m\u001b[1;32m     38\u001b[0m         keras.layers.Embedding(\n\u001b[1;32m     39\u001b[0m             \u001b[0minput_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/appl/soft/ai/miniconda3/envs/tensorflow-1.14.0/lib/python3.7/site-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[1;32m    495\u001b[0m                                    \u001b[0minput_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    496\u001b[0m                                    \u001b[0moutput_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 497\u001b[0;31m                                    arguments=user_kwargs)\n\u001b[0m\u001b[1;32m    498\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    499\u001b[0m             \u001b[0;31m# Apply activity regularizer if any:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/appl/soft/ai/miniconda3/envs/tensorflow-1.14.0/lib/python3.7/site-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m_add_inbound_node\u001b[0;34m(self, input_tensors, output_tensors, input_masks, output_masks, input_shapes, output_shapes, arguments)\u001b[0m\n\u001b[1;32m    563\u001b[0m         \u001b[0;31m# Update tensor history, _keras_shape and _uses_learning_phase.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    564\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_tensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 565\u001b[0;31m             \u001b[0moutput_tensors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_keras_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput_shapes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    566\u001b[0m             uses_lp = any(\n\u001b[1;32m    567\u001b[0m                 [getattr(x, '_uses_learning_phase', False)\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "seq_len = args.max_seq_length    # abbreviation\n",
    "\n",
    "pretrained_model, tokenizer = load_pretrained(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "train_words, train_tags = read_conll(args.train_data)\n",
    "test_words, test_tags = read_conll(args.test_data)\n",
    "train_data = process_sentences(train_words, train_tags, tokenizer, seq_len)\n",
    "test_data = process_sentences(test_words, test_tags, tokenizer, seq_len)\n",
    "\n",
    "label_list = get_labels(train_data.labels)\n",
    "tag_map = { l: i for i, l in enumerate(label_list) }\n",
    "inv_tag_map = { v: k for k, v in tag_map.items() }\n",
    "\n",
    "init_prob, trans_prob = viterbi_probabilities(train_data.labels, tag_map)\n",
    "\n",
    "train_x = encode(train_data.combined_tokens, tokenizer, seq_len)\n",
    "test_x = encode(test_data.combined_tokens, tokenizer, seq_len)\n",
    "\n",
    "train_y, train_weights = label_encode(\n",
    "    train_data.combined_labels, tag_map, seq_len)\n",
    "test_y, test_weights = label_encode(\n",
    "    test_data.combined_labels, tag_map, seq_len)\n",
    "\n",
    "ner_model = create_ner_model(pretrained_model, len(tag_map))\n",
    "optimizer = create_optimizer(len(train_x[0]), args)\n",
    "\n",
    "ner_model.compile(\n",
    "    optimizer,\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    sample_weight_mode='temporal',\n",
    "    metrics=['sparse_categorical_accuracy']\n",
    ")\n",
    "\n",
    "callbacks_list = [\n",
    "keras.callbacks.EarlyStopping(monitor='val_loss',patience=1,),\n",
    "keras.callbacks.ModelCheckpoint(filepath=args.ner_model_dir+'/model.hdf5',\n",
    "monitor='val_loss',save_best_only=True,)]\n",
    "\n",
    "if args.early_stopping:\n",
    "    print(\"NER:Using early stopping.\")\n",
    "    ner_model.fit(\n",
    "    train_x,train_y,sample_weight=train_weights,epochs=args.num_train_epochs,\n",
    "    batch_size=args.batch_size,callbacks=callbacks_list,validation_data=(test_x,test_y))\n",
    "else:\n",
    "    ner_model.fit(train_x,train_y,sample_weight=train_weights,\n",
    "    epochs=args.num_train_epochs,batch_size=args.batch_size,)\n",
    "\n",
    "if args.ner_model_dir is not None:\n",
    "    label_list = [v for k, v in sorted(list(inv_tag_map.items()))]\n",
    "    save_ner_model(ner_model, tokenizer, label_list, args)\n",
    "    save_viterbi_probabilities(init_prob, trans_prob, inv_tag_map, args)\n",
    "\n",
    "probs = ner_model.predict(test_x, batch_size=args.batch_size)\n",
    "preds = np.argmax(probs, axis=-1)\n",
    "\n",
    "pred_tags = []\n",
    "for i, pred in enumerate(preds):\n",
    "    pred_tags.append([inv_tag_map[t]\n",
    "                      for t in pred[1:len(test_data.tokens[i])+1]])\n",
    "\n",
    "lines = write_result(\n",
    "    args.output_file, test_data.words, test_data.lengths,\n",
    "    test_data.tokens, test_data.labels, pred_tags\n",
    ")\n",
    "\n",
    "c = conlleval.evaluate(lines)\n",
    "conlleval.report(c)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
